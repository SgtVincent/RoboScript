{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import sys\n",
    "import numpy as np\n",
    "from scipy.spatial.transform import Rotation as R \n",
    "import rospy \n",
    "import rospkg\n",
    "import jupyros as jr\n",
    "import pprint \n",
    "import matplotlib.pyplot as plt\n",
    "from std_msgs.msg import String, Header\n",
    "from geometry_msgs.msg import PoseStamped, Pose, Point, Quaternion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add catkin_ws context \n",
    "sys.path.append(\"/home/junting/franka_ws/devel/lib/python3.9/site-packages\")\n",
    "\n",
    "from src.lmp import *\n",
    "from src.env.multimodal_env import MultiModalEnv\n",
    "from src.configs.config import load_config\n",
    "\n",
    "cfg_tabletop = load_config(\"perception_few_shot_gpt_3.yaml\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "# initialize environment\n",
    "########################################\n",
    "rospy.init_node('jupyter_multimodal', log_level=rospy.DEBUG)\n",
    "# get package root path \n",
    "pkg_root = rospkg.RosPack().get_path('instruct_to_policy')\n",
    "use_gt_2d_detections = True\n",
    "use_gt_3d_bboxes = True\n",
    "use_gt_planning_scene = False\n",
    "\n",
    "if use_gt_2d_detections:\n",
    "    cfg_tabletop['grounding_model']['model_name'] = 'ground_truth'\n",
    "if use_gt_3d_bboxes:\n",
    "    cfg_tabletop['grounding_model']['model_name'] = 'ground_truth'\n",
    "    \n",
    "cfg_tabletop['perception']['use_gt_2d_detections'] = use_gt_2d_detections\n",
    "cfg_tabletop['perception']['use_gt_3d_bboxes'] = use_gt_3d_bboxes\n",
    "cfg_tabletop['perception']['use_gt_planning_scene'] = use_gt_planning_scene\n",
    "\n",
    "\n",
    "# setup environment\n",
    "env = MultiModalEnv(cfg_tabletop)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.detect_objects([\"orange\", \"cabinet.handle_0\"])\n",
    "env.detect_objects([\"apple\", \"footed_bowl\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grasp_pose = env.parse_adaptive_shape_grasp_pose(\"apple_0\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.open_gripper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.publish_goal_to_marker(grasp_pose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close_gripper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.attach_object(\"apple_0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.scene.visualize_3d_bboxes()\n",
    "env.get_object_name_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get 2d visualization and matched boxes\n",
    "# object_list = [\"orange\", \"toy_bus\", \"basket_with_fabric_liner\"]\n",
    "object_list = [\"apple\", \"footed_bowl\"]\n",
    "sensor_data = env.get_sensor_data()\n",
    "\n",
    "if use_gt_2d_detections or use_gt_3d_bboxes:\n",
    "    gt_bbox_3d_dict = {}\n",
    "    gazebo_gt_bboxes_3d = env.get_gt_bboxes()\n",
    "    for object_name in object_list:\n",
    "        if object_name in gazebo_gt_bboxes_3d:\n",
    "            if \".\" in object_name: # the object is already a part of a model\n",
    "                object_id = object_name\n",
    "            else:\n",
    "                object_id = f\"{object_name}_0\"\n",
    "            # FIXME: consider the case when there are multiple objects with the same name\n",
    "            gt_bbox_3d_dict[object_id] = gazebo_gt_bboxes_3d[object_name]\n",
    "\n",
    "# add the ground truth 3d bboxes to sensor data\n",
    "sensor_data['bbox_3d_dict'] = gt_bbox_3d_dict\n",
    "\n",
    "detections_list = env.grounding_model.query_2d_bbox_list(\n",
    "    sensor_data=sensor_data,\n",
    "    object_list=object_list\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detections_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "import cv2\n",
    "\n",
    "def draw_multiview_bbox_matches(rgb_image_list: List[np.ndarray], bboxes_2d_list: List[List[np.ndarray]], \n",
    "                                matched_bboxes_idx_tuple_list: List[Tuple[np.ndarray]], object_name_list: List[str],\n",
    "                                thickness=5, text_thickness=1, text_scale=0.5) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Draw all 2D bounding box matches across different camera views with object names.\n",
    "    Place all RGB images from top to bottom.\n",
    "    For each tuple of matched bounding boxes across different views, draw them with the same color. Different tuples will have different colors.\n",
    "    Then draw lines between the matched bounding boxes between top-to-bottom images and add object names near the bounding boxes.\n",
    "    \n",
    "    Args:\n",
    "        rgb_image_list (List[np.ndarray]): List of RGB images.\n",
    "        bboxes_2d_list (List[List[np.ndarray]]): List of lists of 2D bounding boxes.\n",
    "        matched_bboxes_idx_tuple_list (List[Tuple[np.ndarray]]): List of tuples of matched 2D bounding box indices.\n",
    "        object_name_list (List[str]): List of object names corresponding to each tuple in matched_bboxes_idx_tuple_list.\n",
    "        thickness (int): Bounding box thickness.\n",
    "        text_thickness (int): Thickness of the text.\n",
    "        text_scale (float): Scale of the text.\n",
    "        \n",
    "    Returns:\n",
    "        np.ndarray: RGB image with bounding boxes and object names.\n",
    "    \"\"\"\n",
    "    num_views = len(rgb_image_list)\n",
    "    max_width = max([rgb_image.shape[1] for rgb_image in rgb_image_list])\n",
    "    total_height = sum([rgb_image.shape[0] for rgb_image in rgb_image_list])\n",
    "    canvas = np.zeros((total_height, max_width, 3), dtype=np.uint8)\n",
    "    \n",
    "    offset = 0\n",
    "    # first draw all RGB images\n",
    "    for view_idx, rgb_image in enumerate(rgb_image_list):\n",
    "        canvas[offset:offset + rgb_image.shape[0], :rgb_image.shape[1]] = rgb_image\n",
    "        offset += rgb_image.shape[0]\n",
    "        \n",
    "    offset = 0\n",
    "    # draw matched bounding boxes, lines, and object names\n",
    "    for view_idx, rgb_image in enumerate(rgb_image_list):\n",
    "        cmap = plt.cm.get_cmap('tab20')  # Choose a colormap\n",
    "        for match_idx, matched_bboxes_idx_tuple in enumerate(matched_bboxes_idx_tuple_list):\n",
    "            color = (np.array(cmap(match_idx % 20)[:3]) * 255).astype(int).tolist()\n",
    "            if matched_bboxes_idx_tuple[view_idx] != -1:\n",
    "                bbox_2d = bboxes_2d_list[view_idx][matched_bboxes_idx_tuple[view_idx]].copy()\n",
    "                bbox_2d[1] += offset\n",
    "                bbox_2d[3] += offset\n",
    "                cv2.rectangle(canvas, (bbox_2d[0], bbox_2d[1]), (bbox_2d[2], bbox_2d[3]), color=color, thickness=thickness)\n",
    "                \n",
    "                # add object name text\n",
    "                text_position = (bbox_2d[0], bbox_2d[1] - 10)  # adjust as needed\n",
    "                cv2.putText(canvas, object_name_list[match_idx], text_position, cv2.FONT_HERSHEY_SIMPLEX, \n",
    "                            text_scale, color=color, thickness=text_thickness)\n",
    "                \n",
    "                if view_idx < num_views - 1 and matched_bboxes_idx_tuple[view_idx + 1] != -1:\n",
    "                    next_bbox_2d = bboxes_2d_list[view_idx + 1][matched_bboxes_idx_tuple[view_idx + 1]].copy()\n",
    "                    next_bbox_2d[1] += offset + rgb_image.shape[0]\n",
    "                    next_bbox_2d[3] += offset + rgb_image.shape[0]\n",
    "\n",
    "                    start_point = ((bbox_2d[0] + bbox_2d[2]) // 2, bbox_2d[3])\n",
    "                    end_point = ((next_bbox_2d[0] + next_bbox_2d[2]) // 2, next_bbox_2d[1])\n",
    "                    cv2.line(canvas, start_point, end_point, color=color, thickness=thickness)\n",
    "        offset += rgb_image.shape[0]\n",
    "\n",
    "    return canvas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from src.perception.utils import draw_multiview_bbox_matches\n",
    "\n",
    "bboxes_2d_list = [\n",
    "    [bboxes_2d for obj_name, bboxes_2d in detections.items()] \n",
    "    for detections in detections_list\n",
    "]\n",
    "\n",
    "matched_bboxes_idx_tuple_list = [\n",
    "    [0, 0, 0],\n",
    "    [1, 1, 1],\n",
    "    [2, 2, 2]\n",
    "]\n",
    "\n",
    "matched_bboxes_idx_tuple_list = [\n",
    "    [0, 0],\n",
    "    [1, 1],\n",
    "    # [2, 2]\n",
    "]\n",
    "\n",
    "\n",
    "canvas = draw_multiview_bbox_matches(\n",
    "    rgb_image_list=sensor_data['rgb_image_list'][:2],\n",
    "    bboxes_2d_list=bboxes_2d_list[:2],\n",
    "    matched_bboxes_idx_tuple_list=matched_bboxes_idx_tuple_list,\n",
    "    object_name_list=['orange', 'toy bus', 'basket'],\n",
    "    text_thickness=2,\n",
    "    text_scale=1\n",
    ")\n",
    "\n",
    "# visualize the canvas without axis\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.axis('off')\n",
    "plt.imshow(canvas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joint_list = env.joint_prediction_model.predict(            \n",
    "            {\n",
    "                \"object_name\": 'cabinet',\n",
    "                \"joint_types\": [\"prismatic\"],\n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import open3d as o3d\n",
    "import numpy as np\n",
    "\n",
    "vis_list = []\n",
    "\n",
    "\n",
    "# [{'joint_position': array([-1.36175061, -0.20400387,  1.162368  ]),\n",
    "#   'joint_axis': array([ 1.00000000e+00, -7.90316541e-06, -4.15183898e-09]),\n",
    "#   'type': 'prismatic'},\n",
    "#  {'joint_position': array([-1.38764469, -0.20400389,  0.90824719]),\n",
    "#   'joint_axis': array([ 1.00000000e+00, -7.90316533e-06, -4.15313506e-09]),\n",
    "#   'type': 'prismatic'},\n",
    "#  {'joint_position': array([-1.38765743, -0.20400391,  0.67294999]),\n",
    "#   'joint_axis': array([ 1.00000000e+00, -7.90316526e-06, -4.03956951e-09]),\n",
    "#   'type': 'prismatic'},\n",
    "#  {'joint_position': array([-1.38764255, -0.20400394,  0.3623576 ]),\n",
    "#   'joint_axis': array([ 1.00000000e+00, -7.90316601e-06, -3.97314492e-09]),\n",
    "#   'type': 'prismatic'}]\n",
    "\n",
    "# Add 3D bounding boxes and object names\n",
    "for obj_name in env.scene.object_names:\n",
    "    bbox_3d = env.scene.bbox_3d_dict[obj_name]\n",
    "\n",
    "    # Create an axis-aligned bounding box\n",
    "    bbox = o3d.geometry.AxisAlignedBoundingBox(\n",
    "        min_bound=np.array(bbox_3d[:3]),\n",
    "        max_bound=np.array(bbox_3d[3:])\n",
    "    )\n",
    "    vis_list.append(bbox)\n",
    "    \n",
    "    vis_list.append(env.scene.scene_tsdf_full.get_mesh())\n",
    "\n",
    "# also visualize the joints as arrows in open3d  \n",
    "for joint in joint_list:\n",
    "    joint_position = joint['joint_position']\n",
    "    joint_axis = joint['joint_axis']\n",
    "    # Create an arrow\n",
    "    arrow = o3d.geometry.TriangleMesh.create_arrow(\n",
    "        cylinder_radius=0.01,\n",
    "        cone_radius=0.02,\n",
    "        cylinder_height=0.5,\n",
    "        cone_height=0.1\n",
    "    )\n",
    "    # Translate the arrow\n",
    "    arrow.translate(joint_position)\n",
    "    \n",
    "    # Rotate the arrow\n",
    "    o3d.geometry.get_rotation_matrix_from_axis_angle(joint_axis)\n",
    "    rotaion_matrix = R.align_vectors([joint_axis], [np.array([0, 0, 1])])[0].as_matrix()\n",
    "    arrow.rotate(rotaion_matrix, center=joint_position)\n",
    "\n",
    "    # change color \n",
    "    arrow.paint_uniform_color([1, 0, 0])\n",
    "        \n",
    "    vis_list.append(arrow)\n",
    "    \n",
    "# Visualize the scene\n",
    "o3d.visualization.draw_geometries(vis_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.get_object_name_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import open3d as o3d\n",
    "vis_list = []\n",
    "for obj_name in ['apple_0', \"footed_bowl_0\"]:\n",
    "    bbox_3d = env.scene.bbox_3d_dict[obj_name]\n",
    "    # create a axis-aligned bounding box \n",
    "    bbox = o3d.geometry.AxisAlignedBoundingBox(\n",
    "        min_bound=np.array(bbox_3d[:3]),\n",
    "        max_bound=np.array(bbox_3d[3:])\n",
    "    )\n",
    "    vis_list.append(bbox)\n",
    "    \n",
    "vis_list.append(env.scene.scene_tsdf_full.get_mesh())\n",
    "\n",
    "# visualize the scene\n",
    "o3d.visualization.draw_geometries(vis_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get gt bboxes: \n",
    "gt_bboxes_dict = env.get_gt_bboxes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint.pprint(gt_bboxes_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import open3d as o3d \n",
    "\n",
    "# get all handle center position \n",
    "handles = [\"cabinet::handle_0\", \"cabinet::handle_1\"]\n",
    "\n",
    "handle_centers = [\n",
    "    gt_bboxes_dict[handle][0] for handle in handles\n",
    "]\n",
    "\n",
    "vis_list = []\n",
    "vis_list.append(env.scene.scene_tsdf_full.get_mesh())\n",
    "\n",
    "# visualize handle center positions as a small sphere in open3d \n",
    "for handle_center in handle_centers:\n",
    "    handle_center = np.array(handle_center)\n",
    "    sphere = o3d.geometry.TriangleMesh.create_sphere(radius=0.01)\n",
    "    sphere.translate(handle_center)\n",
    "    vis_list.append(sphere) \n",
    "\n",
    "o3d.visualization.draw_geometries(vis_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.add_scene_objects_to_moveit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grasp_pose = env.parse_adaptive_shape_grasp_pose('apple_0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.planning_scene.get_known_object_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.open_gripper()\n",
    "env.grasp(grasp_pose)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close_gripper()\n",
    "env.attach_object('apple_0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "place_pose = env.parse_place_pose('apple_0', 'plate_0')\n",
    "env.move_to_pose(place_pose)\n",
    "env.open_gripper()\n",
    "env.detach_object('apple_0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# read list of depth images, clip and replot to gray scale image\n",
    "paths = [\n",
    "    \"/home/junting/catkin_ws/src/RoboScript/instruct_to_policy/data/benchmark/multiview_detection/depth_images/world_2_pick_and_place_camera_left.png\",\n",
    "    \"/home/junting/catkin_ws/src/RoboScript/instruct_to_policy/data/benchmark/multiview_detection/depth_images/world_2_pick_and_place_camera_right.png\",\n",
    "    \"/home/junting/catkin_ws/src/RoboScript/instruct_to_policy/data/benchmark/multiview_detection/depth_images/world_2_pick_and_place_camera_top.png\"\n",
    "]\n",
    "\n",
    "depth_images = []\n",
    "for path in paths:\n",
    "    depth_image = cv2.imread(path, cv2.IMREAD_UNCHANGED)\n",
    "    depth_images.append(depth_image)\n",
    "\n",
    "# clamp depth image to 0-5\n",
    "for i, depth_image in enumerate(depth_images):\n",
    "    depth_images[i] = np.clip(depth_image, 100, 5000)\n",
    "\n",
    "# # re-scale depth image to 0-255\n",
    "# for i, depth_image in enumerate(depth_images):\n",
    "#     depth_images[i] = (depth_image / 5 * 255).astype(np.uint8)\n",
    "\n",
    "# plot the depth images\n",
    "# fig, axes = plt.subplots(1, len(depth_images), figsize=(12, 4))\n",
    "# for i, depth_image in enumerate(depth_images):\n",
    "#     axes[i].imshow(depth_image, cmap='gray')\n",
    "#     axes[i].axis('off')\n",
    "# plt.show()\n",
    "\n",
    "# # plot the depth images one by one \n",
    "for depth_image in depth_images:\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(depth_image, cmap='gray')\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot rgb images one by one \n",
    "paths = [\n",
    "    \"/home/junting/catkin_ws/src/RoboScript/instruct_to_policy/data/benchmark/multiview_detection/rgb_images/world_2_pick_and_place_camera_left.png\",\n",
    "    \"/home/junting/catkin_ws/src/RoboScript/instruct_to_policy/data/benchmark/multiview_detection/rgb_images/world_2_pick_and_place_camera_right.png\",\n",
    "    \"/home/junting/catkin_ws/src/RoboScript/instruct_to_policy/data/benchmark/multiview_detection/rgb_images/world_2_pick_and_place_camera_top.png\"\n",
    "]\n",
    "\n",
    "for path in paths:\n",
    "    rgb_image = cv2.imread(path)\n",
    "    rgb_image = cv2.cvtColor(rgb_image, cv2.COLOR_BGR2RGB)\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(rgb_image)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ros_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
