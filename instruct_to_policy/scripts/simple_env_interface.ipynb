{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n",
      "Failed to import pyassimp, see https://github.com/ros-planning/moveit/issues/86 for more info\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import sys\n",
    "import numpy as np\n",
    "import argparse\n",
    "from scipy.spatial.transform import Rotation as R \n",
    "# add catkin_ws context \n",
    "sys.path.append(\"/home/junting/franka_ws/devel/lib/python3.9/site-packages\")\n",
    "\n",
    "from src.lmp import *\n",
    "from src.env.true_grounding_env import TrueGroundingEnv\n",
    "from src.config import cfg_tabletop\n",
    "import rospy \n",
    "import rospkg\n",
    "import jupyros as jr\n",
    "\n",
    "from std_msgs.msg import String, Header\n",
    "from geometry_msgs.msg import PoseStamped, Pose, Point, Quaternion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] [1702272761.538526, 0.000000]: init_node, name[/eval_code], pid[4100086]\n",
      "[DEBUG] [1702272761.540126, 0.000000]: binding to 0.0.0.0 0\n",
      "[DEBUG] [1702272761.541535, 0.000000]: bound to 0.0.0.0 35993\n",
      "[DEBUG] [1702272761.542083, 0.000000]: connecting to junting-PC 57917\n",
      "[DEBUG] [1702272761.543612, 0.000000]: ... service URL is rosrpc://junting-PC:35993\n",
      "[DEBUG] [1702272761.545246, 0.000000]: [/eval_code/get_loggers]: new Service instance\n",
      "[DEBUG] [1702272761.548796, 579.535000]: ... service URL is rosrpc://junting-PC:35993\n",
      "[DEBUG] [1702272761.550149, 579.537000]: [/eval_code/set_logger_level]: new Service instance\n",
      "[INFO] [1702272761.700874, 579.670000]: camera_left: Waiting for camera_left/color/camera_info...\n",
      "[DEBUG] [1702272761.702085, 579.671000]: connecting to junting-PC 37585\n",
      "[DEBUG] [1702272761.703232, 579.672000]: connecting to junting-PC 57917\n",
      "[DEBUG] [1702272761.718603, 579.686000]: connecting to junting-PC 57917\n",
      "[DEBUG] [1702272761.721860, 579.689000]: connecting to junting-PC 37585\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] [1702272761.752522, 579.718000]: connecting to junting-PC 57917\n",
      "[INFO] [1702272762.235990, 580.171000]: camera_left: camera_left/color/camera_info received!\n",
      "[INFO] [1702272762.238053, 580.172000]: camera_left: camera_left/depth/camera_info received!\n",
      "[DEBUG] [1702272762.245301, 580.180000]: connecting to junting-PC 57917\n",
      "[INFO] [1702272762.248141, 580.182000]: camera_right: Waiting for camera_right/color/camera_info...\n",
      "[DEBUG] [1702272762.251431, 580.186000]: connecting to junting-PC 55655\n",
      "[DEBUG] [1702272762.254281, 580.189000]: connecting to junting-PC 57917\n",
      "[DEBUG] [1702272762.256005, 580.190000]: connecting to junting-PC 55655\n",
      "[INFO] [1702272762.773309, 580.686000]: camera_right: camera_right/color/camera_info received!\n",
      "[INFO] [1702272762.774751, 580.687000]: camera_right: camera_right/depth/camera_info received!\n",
      "[DEBUG] [1702272762.780833, 580.693000]: connecting to junting-PC 57917\n",
      "[DEBUG] [1702272762.783682, 580.695000]: connecting to junting-PC 37631\n",
      "[INFO] [1702272762.784062, 580.695000]: camera_top: Waiting for camera_top/color/camera_info...\n",
      "[DEBUG] [1702272762.787277, 580.698000]: connecting to junting-PC 57917\n",
      "[DEBUG] [1702272762.788605, 580.700000]: connecting to junting-PC 37631\n",
      "[INFO] [1702272763.317366, 581.198000]: camera_top: camera_top/color/camera_info received!\n",
      "[INFO] [1702272763.318794, 581.199000]: camera_top: camera_top/depth/camera_info received!\n",
      "[DEBUG] [1702272763.322889, 581.202000]: ... service URL is rosrpc://junting-PC:35993\n",
      "[DEBUG] [1702272763.324057, 581.203000]: [/eval_code/tf2_frames]: new Service instance\n",
      "\u001b[0m[ INFO] [1702272763.354539312]: Loading robot model 'panda'...\u001b[0m\n",
      "[DEBUG] [1702272763.711508, 581.555000]: connecting to ('junting-PC', 38433)\n",
      "[DEBUG] [1702272763.715920, 581.556000]: connecting to junting-PC 46487\n",
      "\u001b[0m[ INFO] [1702272764.714799159, 582.436000000]: Ready to take commands for planning group panda_manipulator.\u001b[0m\n",
      "\u001b[0m[ INFO] [1702272765.667095569, 583.193000000]: Ready to take commands for planning group panda_arm.\u001b[0m\n",
      "[DEBUG] [1702272765.669325, 581.567000]: connecting to junting-PC 33305\n",
      "[DEBUG] [1702272765.681002, 583.203000]: connecting to junting-PC 57917\n",
      "[DEBUG] [1702272765.684529, 583.210000]: connecting to junting-PC 58255\n",
      "[DEBUG] [1702272765.686864, 583.212000]: connecting to junting-PC 46487\n",
      "[DEBUG] [1702272765.688641, 583.214000]: connecting to junting-PC 33305\n",
      "[DEBUG] [1702272765.731694, 583.251000]: connecting to junting-PC 57917\n",
      "[DEBUG] [1702272765.734020, 583.253000]: connecting to junting-PC 57917\n",
      "[DEBUG] [1702272765.735927, 583.255000]: connecting to junting-PC 57917\n",
      "[DEBUG] [1702272765.953516, 583.473000]: connecting to junting-PC 57917\n",
      "[DEBUG] [1702272765.960588, 583.478000]: connecting to junting-PC 57917\n",
      "[DEBUG] [1702272765.961941, 583.481000]: connecting to junting-PC 57917\n",
      "[DEBUG] [1702272766.226387, 583.724000]: connecting to junting-PC 57917\n",
      "[DEBUG] [1702272766.227656, 583.726000]: connecting to junting-PC 57917\n",
      "[DEBUG] [1702272766.229085, 583.727000]: connecting to junting-PC 57917\n",
      "Gripper action clients ready\n",
      "Set up Franka API. Ready to go!\n",
      "[DEBUG] [1702272766.526748, 584.022000]: connecting to ('junting-PC', 41753)\n",
      "[INFO] [1702272766.527979, 584.023000]: Grasp detection: remote model service ready\n"
     ]
    }
   ],
   "source": [
    "########################################\n",
    "# initialize environment\n",
    "########################################\n",
    "rospy.init_node('eval_code', log_level=rospy.DEBUG)\n",
    "# get package root path \n",
    "pkg_root = rospkg.RosPack().get_path('instruct_to_policy')\n",
    "\n",
    "# setup environment\n",
    "env = TrueGroundingEnv(cfg_tabletop)\n",
    "# env.reset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3D fusion \n",
    "from src.perception.scene_manager import SceneManager\n",
    "sensor_data = env.get_sensor_data()\n",
    "sensor_data['detections_list'] = [{},{},{}]\n",
    "\n",
    "scene_manager = SceneManager()\n",
    "scene_manager.update_fusion(sensor_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get cropped point cloud\n",
    "drawer_bbox = env.get_3d_bbox('cabinet.drawer0')\n",
    "drawer_pcd = scene_manager.scene_tsdf_full.crop_cloud(\n",
    "    crop_center=(drawer_bbox[:3] + drawer_bbox[3:]) / 2,\n",
    "    crop_size=(drawer_bbox[3:] - drawer_bbox[:3])\n",
    ")\n",
    "cabinet_bbox = env.get_3d_bbox('cabinet')\n",
    "cabinet_pcd = scene_manager.scene_tsdf_full.crop_cloud(\n",
    "    crop_center=(cabinet_bbox[:3] + cabinet_bbox[3:]) / 2,\n",
    "    crop_size=(cabinet_bbox[3:] - cabinet_bbox[:3])\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import open3d as o3d \n",
    "# o3d.io.write_point_cloud(os.path.join(pkg_root, 'data', 'drawer.pcd'), drawer_pcd)\n",
    "# o3d.io.write_point_cloud(os.path.join(pkg_root, 'data', 'cabinet.pcd'), cabinet_pcd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] [1702272769.572470, 586.685000]: Sending perception data to grasp detection service\n",
      "[DEBUG] [1702272769.575478, 586.701000]: connecting to junting-PC 41753\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "position: \n",
       "  x: -0.2738809883594513\n",
       "  y: -0.22210144996643066\n",
       "  z: 1.0694431066513062\n",
       "orientation: \n",
       "  x: 0.6993529206633681\n",
       "  y: -0.6677615559119899\n",
       "  z: 0.1843941981573167\n",
       "  w: 0.17606469405174707"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.parse_adaptive_shape_grasp_pose(\"apple\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load files & data \n",
    "def filter_drawer0(processed_data, raw_data, mask):\n",
    "    \"\"\"\n",
    "    Filter tasks involving interacting with drawer 0, since it is too far from the robot.\n",
    "    \"\"\"\n",
    "\n",
    "    for i, data in enumerate(processed_data):\n",
    "        if mask[i] == 0: # already filtered in previous filters \n",
    "            continue\n",
    "        full_query = data['query']\n",
    "        instruction = full_query.split(';')[-1]\n",
    "        if 'drawer0' in instruction:\n",
    "            mask[i] = 0\n",
    "            \n",
    "    return mask \n",
    "\n",
    "\n",
    "def filter_tasks(processed_data, raw_data):\n",
    "    \"\"\"\n",
    "    Filter the tasks that are not suitable for the environment based on hand-crafted rules.\n",
    "    \"\"\"\n",
    "    filter_funcs = [filter_drawer0]\n",
    "    mask = np.ones((len(processed_data)))\n",
    "    for func in filter_funcs:\n",
    "        mask = func(processed_data, raw_data, mask)\n",
    "        \n",
    "    filtered_processed_data = [processed_data[i] for i in range(len(processed_data)) if mask[i] == 1]\n",
    "    filtered_raw_data = [raw_data[i] for i in range(len(raw_data)) if mask[i] == 1]\n",
    "    return filtered_processed_data, filtered_raw_data\n",
    "\n",
    "processed_file_path = os.path.join(pkg_root, processed_file)\n",
    "raw_file_path = os.path.join(pkg_root, raw_file)\n",
    "\n",
    "with open(processed_file_path, 'r') as f:\n",
    "    processed_data = json.load(f)\n",
    "with open(raw_file_path, 'r') as f:\n",
    "    raw_data = json.load(f)\n",
    "\n",
    "# filter tasks that are not suitable for the environment\n",
    "filtered_processed_data, filtered_raw_data = filter_tasks(processed_data, raw_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pick and Place"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pose_msg = env.parse_grasp_pose(\"apple\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rot_mat = R.from_quat([pose_msg.orientation.x, pose_msg.orientation.y, pose_msg.orientation.z, pose_msg.orientation.w]).as_matrix()\n",
    "translation = np.array([pose_msg.position.x, pose_msg.position.y, pose_msg.position.z])\n",
    "\n",
    "depth = 0.05\n",
    "\n",
    "pregrasp_offset_local = np.array([0, 0, -0.15]).astype(np.float32)\n",
    "# predicted gripper center is 0.02m above the gripper tip\n",
    "approach_offset_local = np.array([0, 0, depth - 0.02 ]).astype(np.float32)\n",
    "pregrasp_position = translation + rot_mat @ pregrasp_offset_local\n",
    "approach_position = translation + rot_mat @ approach_offset_local\n",
    "\n",
    "pregrasp_pose = Pose(position=Point(*pregrasp_position), orientation=pose_msg.orientation)\n",
    "approach_pose = Pose(position=Point(*approach_position), orientation=pose_msg.orientation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.open_gripper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.publish_goal_to_marker(pregrasp_pose)\n",
    "env.move_to_pose(pregrasp_pose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.publish_goal_to_marker(approach_pose)\n",
    "env.move_to_pose(approach_pose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close_gripper(width=0.05, force=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.attach_object(\"apple\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "place_pose = env.parse_place_pose(object_name=\"apple\", receptacle_name=\"white_ceramic_plate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.move_to_pose(place_pose)\n",
    "env.open_gripper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.detach_object(\"apple\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open Drawer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jr.publish('/rviz/moveit/move_marker/goal_panda_hand_tcp', PoseStamped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "# env.open_gripper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.get_obj_name_list()\n",
    "# [bbox.object_id for bbox in env.gazebo_gt_bboxes]\n",
    "env.get_3d_bbox(\"cabinet.handle_0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grasp_pose = env.parse_grasp_pose(object_name=\"cabinet.handle_0\")\n",
    "print(grasp_pose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.publish_goal_to_marker(grasp_pose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.grasp(grasp_pose)\n",
    "# env.move_to_pose(grasp_pose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close_gripper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a horizontal trajectory to open the drawer\n",
    "grasp_position = np.array([grasp_pose.position.x, grasp_pose.position.y, grasp_pose.position.z])\n",
    "pull_position = grasp_position + np.array([0.2, 0, 0]).astype(float)\n",
    "pull_pose = Pose(position=Point(*pull_position), orientation=grasp_pose.orientation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.move_to_pose(pull_pose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.open_gripper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pose_msg = env.parse_grasp_pose(object_name=\"apple\")\n",
    "\n",
    "rot_mat = R.from_quat([pose_msg.orientation.x, pose_msg.orientation.y, pose_msg.orientation.z, pose_msg.orientation.w]).as_matrix()\n",
    "translation = np.array([pose_msg.position.x, pose_msg.position.y, pose_msg.position.z])\n",
    "\n",
    "depth = 0.05\n",
    "\n",
    "pregrasp_offset_local = np.array([0, 0, -0.15]).astype(np.float32)\n",
    "# predicted gripper center is 0.02m above the gripper tip\n",
    "approach_offset_local = np.array([0, 0, depth - 0.02 ]).astype(np.float32)\n",
    "pregrasp_position = translation + rot_mat @ pregrasp_offset_local\n",
    "approach_position = translation + rot_mat @ approach_offset_local\n",
    "\n",
    "pregrasp_pose = Pose(position=Point(*pregrasp_position), orientation=pose_msg.orientation)\n",
    "approach_pose = Pose(position=Point(*approach_position), orientation=pose_msg.orientation)\n",
    " \n",
    "env.open_gripper()\n",
    " \n",
    "# env.publish_goal_to_marker(pregrasp_pose)\n",
    "env.move_to_pose(pregrasp_pose)\n",
    "# env.publish_goal_to_marker(approach_pose)\n",
    "env.move_to_pose(approach_pose)\n",
    "env.close_gripper(width=0.05, force=30)\n",
    "env.attach_object(\"apple\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ROS Python 3 (ipykernel)",
   "language": "python",
   "name": "ros_python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
